from ..CeleryApp import app
from ..DTO import *
import cv2
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from PIL import Image
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
import math
import json
from datetime import datetime, timedelta
from pathlib import Path
import time

# ä»¥æ­¤æª”æ¡ˆç‚ºéŒ¨é»ï¼Œè€Œé CWD
HERE = Path(__file__).resolve().parent           # .../tasks
ROOT = HERE.parent                               # å°ˆæ¡ˆæ ¹ï¼ˆå« promptsã€tasks çš„é‚£å±¤ï¼‰
PROMPTS_DIR = ROOT / "prompts"
SYSTEM_PROMPT_PATH = PROMPTS_DIR / "system_prompt.md"


# SSIM éœ€ç”¨ skimageï¼›åªæœ‰åœ¨ module="SSIM" æ™‚æ‰æœƒç”¨åˆ°
try:
    from skimage.metrics import structural_similarity as ssim
    _HAS_SKIMAGE = True
except Exception:
    _HAS_SKIMAGE = False

# è¨ˆæ™‚è£é£¾å™¨
def timer(func):
    
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"Function {func.__name__} took {end_time - start_time:.2f} seconds")
        return result
    return wrapper

@timer
def get_video_frames(video_url: str, target_fps: int = 3):
    video_info = {
                "video_url": video_url,
                "fps": None,
                "duration": None,  # å½±ç‰‡ç¸½é•·åº¦ï¼ˆç§’ï¼‰
                "total_frames": None,
                "target_frame": None,
                "possible_extracts": None,
                "extracted_frames": None
            }
    if target_fps <= 0:
        raise ValueError("target_fps å¿…é ˆæ˜¯æ­£æ•¸ï¼Œä¸”å¤§æ–¼0")
    
    cap = cv2.VideoCapture(video_url)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps * 1000  # æ¯«ç§’

    # æª¢æŸ¥å£æª”èˆ‡target_fps = 0 çš„é›–å°æƒ…æ³
    if fps <= 0 or total_frames <= 0:
        cap.release()
        raise ValueError(f"å½±ç‰‡æª”æ¡ˆç„¡æ³•æ­£ç¢ºè®€å–æˆ–ç¸½å¹€ç‚º0,target_fps: {target_fps}, video_original_fps: {fps}, total_frames: {total_frames}")
    if target_fps <= 0:
        raise ValueError("target_fps å¿…é ˆå¤§æ–¼0")
    
    target_frame_interval = int(fps / target_fps) # ä¼°ç®—
    if target_frame_interval <= 1:
        cap.release()
        raise ValueError(f"target_fps:{target_fps}éé«˜ï¼Œè¶…éå½±ç‰‡åŸå§‹fps:{fps}ï¼Œç„¡æ³•æŠ½å–å¹€ï¼Œè«‹èª¿æ•´ç‚ºæ›´ä½çš„å€¼")
    interval_ms = 1000 / target_fps 
    # è¨ˆç®—å½±ç‰‡ç†è«–å¯æŠ½å–çš„å¼µæ•¸ (å¿½ç•¥æœ€å¾Œä¸è¶³ interval çš„å¹€)
    
    video_info.update({
        "fps": fps,
        "total_frames": total_frames,
        "target_frame": target_fps,
    }) #æˆ‘è¦ºå¾—æ•´ç†èµ·ä¾†æ¯”è¼ƒå¥½é–±è®€ï¼Œæœ‰èª°ç¶­è­·çœ‹ä¸çˆ½å¯ä»¥æ”¹æ‰

    # é–‹å§‹æ­£å¼æŠ½å¹€
    output_count = 0  # è¨˜éŒ„å·²è¼¸å‡ºå¹¾å¼µ
    current_ms = 0.0
    frames_dicts = []
    
    while current_ms <= duration:
        cap.set(cv2.CAP_PROP_POS_MSEC, current_ms)
        ret, frame = cap.read()
        if not ret:
            break

        frames_dicts.append({
            "stamp": current_ms / 1000.0,  # ç§’æ•¸
            "frame": frame
        })

        output_count += 1
        current_ms += interval_ms

    cap.release()
    video_info.update({
        "extracted_frames": output_count,
        "duration": duration / 1000.0,  # ç§’æ•¸
        })
    return {
        "video_info": video_info,
        "frames": frames_dicts
    }

@timer
def get_video_frames_fast(video_url: str, target_fps: int = 3):
    """
    GPTæ”¹æˆ‘ç¨‹å¼çš„åŠ é€Ÿç‰ˆ
    """
    if target_fps <= 0:
        raise ValueError("target_fps å¿…é ˆæ˜¯æ­£æ•¸ï¼Œä¸”å¤§æ–¼0")

    cap = cv2.VideoCapture(video_url)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if fps <= 0 or total_frames <= 0:
        cap.release()
        raise ValueError(f"å½±ç‰‡æª”æ¡ˆç„¡æ³•æ­£ç¢ºè®€å–æˆ–ç¸½å¹€ç‚º0, target_fps: {target_fps}, video_original_fps: {fps}, total_frames: {total_frames}")

    # ä»¥ã€Œå¹€ã€ç‚ºå–®ä½è¨ˆç®—æŠ½æ¨£æ­¥é•·ï¼ˆé¿å…æ™‚é–“åˆ¶å°è‡´é‡è§£ç¢¼ï¼‰
    step = max(1, int(round(fps / target_fps)))  # æ¯æŠ“ä¸€å¼µè¦è·³éå¹¾å¹€
    effective_fps = fps / step  # å¯¦éš›æŠ½åˆ°çš„ fpsï¼ˆå¯èƒ½ç•¥ä½æ–¼ target_fpsï¼‰

    frames = []
    kept = 0
    idx = 0

    # åªé †åºè®€å–ï¼Œä¸åš set/seek
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # åªä¿ç•™éœ€è¦çš„å¹€ï¼ˆä»¥ idx åšå–æ¨£ï¼‰
        if idx % step == 0:
            # ç”¨å¹€ç´¢å¼•æ¨ç®—æ™‚é–“æˆ³ï¼ˆä»¥ç§’ï¼‰
            ts = idx / fps
            frames.append({"stamp": ts, "frame": frame})
            kept += 1

        idx += 1

    duration_sec = total_frames / fps
    cap.release()

    video_info = {
        "video_url": video_url,
        "fps": fps,
        "duration": duration_sec,
        "total_frames": total_frames,
        "target_frame": target_fps,
        "possible_extracts": math.floor(total_frames / step),
        "extracted_frames": kept,
        "effective_fps": effective_fps
    }
    return {"video_info": video_info, "frames": frames}
@timer
def analyze_blur(
    frames_dicts: List[Dict[str, Any]],
    threshold: float = 20.0
) -> List[Dict[str, Any]]:
    """
    é‡å° frames_dictï¼ˆ[{ 'stamp': float, 'frame': np.ndarray(BGR) }, ...]ï¼‰è¨ˆç®—æ¸…æ™°åº¦ã€‚
    ä½¿ç”¨ Laplacian è®Šç•°æ•¸ä½œç‚ºæŒ‡æ¨™ï¼Œä½æ–¼ threshold è¦–ç‚ºæ¨¡ç³Šã€‚

    å›å‚³ï¼šlist[dict]ï¼Œæ¯å€‹å…ƒç´ çµæ§‹ç‚º
        {
            "stamp": <float>,
            "frame": <np.ndarray (BGR)>,
            "variance": <float>,          # Laplacian è®Šç•°æ•¸
            "is_not_blurry": <bool>           # è®Šç•°æ•¸ < é–€æª» -> False
        }
    """
    analyzed = []

    for item in frames_dicts:
        # é è¨­å€¼ï¼†åŸºæœ¬æª¢æŸ¥
        stamp = None
        frame = None
        variance = np.nan
        is_blurry = True  # ç„¡æ•ˆè³‡æ–™ä¸€å¾‹è¦–ç‚ºæ¨¡ç³Šï¼Œè®“å¾ŒçºŒå®¹æ˜“éæ¿¾æ‰

        if isinstance(item, dict):
            stamp = float(item.get("stamp")) if item.get("stamp") is not None else None
            frame = item.get("frame")

        if frame is not None and hasattr(frame, "shape"):
            # gray + Laplacian
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if frame.ndim == 3 else frame
            lap = cv2.Laplacian(gray, cv2.CV_64F)
            variance = float(lap.var())
            is_blurry = variance <= threshold # å°æ–¼é–€æª»è¦–ç‚ºæ¨¡ç³Š

        analyzed.append({
            "stamp": stamp,
            "frame": frame,
            "variance": variance,
            "is_not_blurry": not is_blurry # é€™è£¡çš„ is_not_blurry åè½‰äº†é‚è¼¯ï¼ŒTrue è¡¨ç¤ºæ¸…æ™°
        })

    return analyzed

@timer
def filter_by_frame_difference(
    frames_dicts: List[Dict[str, Any]],
    threshold: float = 0.8,
    compression_proportion: float = 0.5,
    module: str = "SSIM"):
    """
    module:
    "MSE_L2" : ç°éšå‡æ–¹å·®ï¼Œ"({n}-({n-1}))^2 if n < 1"
    "SSIM" : çµæ§‹ç›¸ä¼¼åº¦ï¼Œ{\displaystyle {\text{SSIM}}(\mathbf {x} ,\mathbf {y} )=
    [l(\mathbf {x} ,\mathbf {y} )]^{\alpha }
    [c(\mathbf {x} ,\mathbf {y} )]^{\beta }
    [s(\mathbf {x} ,\mathbf {y} )]^{\gamma }}(ç¶­åŸºæŠ„ä¸‹ä¾†çš„ï¼Œé‚„æœ‰ä¸€å †æ²’æœ‰æŠ„ï¼Œå¥½å¥‡çš„è‡ªå·±å»æŸ¥)

    step 1 : å°‡å½±åƒå£“ç¸®ï¼Œä¸¦å½¢æˆå°æ‡‰çš„ key -> stamp ; value -> frame
    step 2 : æ éç¬¬ä¸€å¼µï¼Œå¾ç¬¬äºŒå¼µé–‹å§‹ï¼Œèˆ‡å‰ä¸€å¼µåšå·®ç•°æ¯”å°ï¼Œä¸¦æ ¹æ“škey ä¿®æ”¹åŸå§‹dictçš„åƒæ•¸ 
    """
    if module not in ["MSE_L2", "SSIM"]:
        raise ValueError("module å¿…é ˆæ˜¯ 'MSE_L2' æˆ– 'SSIM'")
    if module == "SSIM" and not _HAS_SKIMAGE:
        raise ImportError("ä½¿ç”¨ SSIM éœ€è¦å®‰è£ scikit-imageï¼špip install scikit-image")

    compression_frames = [
        {   "stamp": item["stamp"],
            "frame": cv2.resize(
                        cv2.cvtColor(item["frame"], cv2.COLOR_BGR2GRAY) if item["frame"].ndim == 3 else item["frame"],
                        (0, 0),
                        fx=compression_proportion,
                        fy=compression_proportion
                    )
        }
        for item in frames_dicts.copy()
     ] # è¤‡è£½çš„ frames_dictsï¼Œé¿å…ä¿®æ”¹åŸå§‹è³‡æ–™ï¼Œä¸¦å£“ç¸®å½±åƒä½œé‹ç®—
    
    filtered_frames = [] #è™•ç†éçš„é™£åˆ—
    for idx in range(len(compression_frames)):  # â† ç´¢å¼•ä¿®æ­£
        if idx == 0:
            filtered_item = frames_dicts[idx].copy()
            filtered_item["ssim_value"] = 0 if module == "SSIM" else None
            filtered_item["mse_value"] = 0 if module == "MSE_L2" else None
            filtered_item["is_significant"] = True
            filtered_frames.append(filtered_item)
            continue

        current_frame = compression_frames[idx]["frame"]
        previous_frame = compression_frames[idx - 1]["frame"]
        diff_value = 0
        ssim_value = 0
        is_significant = False
        if module == "MSE_L2":
            
            diff_value = np.mean((current_frame - previous_frame) ** 2)
            is_significant = diff_value >= threshold
        elif module == "SSIM":
            # using skimage
            ssim_value = ssim(current_frame, previous_frame, data_range=255)
            # è¶Šå¤§è¶Šç›¸ä¼¼ï¼Œæˆ‘å€‘è¦å‰ƒé™¤ç›¸ä¼¼ï¼Œæ‰€ä»¥å°æ–¼é–€æª»çš„è¦–ç‚ºé‡è¦å¹€
            is_significant = ssim_value <= threshold

        filtered_item = frames_dicts[idx].copy()
        filtered_item["ssim_value"] = ssim_value if module == "SSIM" else None
        filtered_item["mse_value"] = diff_value if module == "MSE_L2" else None
        filtered_item["is_significant"] = bool(is_significant)
        filtered_frames.append(filtered_item)

    return filtered_frames  # è¿”å›è™•ç†å¾Œçš„å¹€åˆ—è¡¨ï¼ŒåŒ…å«æ˜¯å¦é¡¯è‘—çš„æ¨™è¨˜


    # è™•ç†æ–¹å¼è¦åŠƒ2,å¦‚éé€Ÿåº¦å¤ªæ…¢å†å›ä¾†åš
    # A = [0,1,2,3,4,....]
    # B = [ ,0,1,2,3,....]
    # A - B 

class BLIPImageCaptioner:
    
    def __init__(self, model_name="Salesforce/blip-image-captioning-base", device=None):
        # print(f"ğŸ” æ­£åœ¨è¼‰å…¥ BLIP æ¨¡å‹ï¼š{model_name}")
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")

        self.processor = BlipProcessor.from_pretrained(model_name)
        self.model = BlipForConditionalGeneration.from_pretrained(model_name)
        self.model.to(self.device)

        # print(f"âœ… BLIP æ¨¡å‹å·²è¼‰å…¥è‡³ {self.device}ã€‚")

    def describe(self, image_input):
        """
        å°‡åœ–åƒè½‰ç‚ºè‡ªç„¶èªè¨€æ•˜è¿°ã€‚

        Args:
            image_input: åœ–ç‰‡è·¯å¾‘ï¼ˆstrï¼‰æˆ– PIL.Image å°è±¡
            prompt: çµ¦æ¨¡å‹çš„æŒ‡ä»¤æç¤ºè©ï¼ˆBLIP-base ä¸éœ€è¦ï¼Œå¯ç‚º Noneï¼‰

        Returns:
            caption: åœ–åƒæè¿°æ–‡å­—ï¼ˆstrï¼‰
        """
        if isinstance(image_input, str):
            image = Image.open(image_input).convert("RGB")
        elif isinstance(image_input, Image.Image):
            image = image_input
        else:
            raise TypeError("è«‹æä¾›åœ–ç‰‡è·¯å¾‘æˆ– PIL Image ç‰©ä»¶")

        inputs = self.processor(image, return_tensors="pt").to(self.device)
        generated_ids = self.model.generate(**inputs, max_new_tokens=50)
        caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)

        return caption
    
# åˆå§‹åŒ– BLIP Captioner 
CAPTIONER = BLIPImageCaptioner()

@timer
def img_captioning(frames_dicts: List[Dict[str, Any]],
                   ):
    for item in frames_dicts:
        if item["is_not_blurry"] and item["is_significant"]:
            frame = item["frame"]

            # OpenCV (BGR) -> PIL (RGB)
            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(image_rgb)

            # ä¸Ÿé€² BLIP ç”¢ç”Ÿæè¿°
            caption = CAPTIONER.describe(pil_image)

            # å­˜å› dict
            item["caption"] = caption
        else: 
            item["caption"] = "<skipped due to blur or insignificance>"
    return frames_dicts

from ..libs.ModelLoad import llm_core
import re
LLM_CORE = llm_core(supplier="google",
                    model_name="gemini-2.0-flash",
                    api_key="AIzaSyBvBotMRaGYMi4YYehNTT80d5-oknnp-68")

def clean_model_output(model_output: str):
    """
    å°‡ Google æ¨¡å‹å›å‚³çš„å­—ä¸²ï¼ˆå« ```json ... ``` æ¨™è¨˜ï¼‰è½‰æˆ Python dict
    """
    # ç§»é™¤ ```json èˆ‡ ```
    cleaned = re.sub(r"^```json|```$", "", model_output.strip(), flags=re.MULTILINE).strip()
    
    # å˜—è©¦è½‰æ›æˆ JSON
    try:
        data = json.loads(cleaned)
        return data
    except json.JSONDecodeError:
        return None
@timer
def llm_processing(frames_dicts: List[Dict[str, Any]],
                   number_of_trys: int = 3):
    #éŒ¯èª¤æª¢æŸ¥
    if not frames_dicts or not isinstance(frames_dicts, list):
        raise ValueError("frames_dicts å¿…é ˆæ˜¯éç©ºçš„åˆ—è¡¨")
    if number_of_trys < 1:
        raise ValueError("number_of_trys å¿…é ˆå¤§æ–¼ç­‰æ–¼1")  

    # åš´æ ¼ç¢ºèªæª”æ¡ˆå­˜åœ¨ï¼Œä¸å­˜åœ¨å°±æ—©é»å ±éŒ¯å¥½æ’æŸ¥
    if not SYSTEM_PROMPT_PATH.is_file():
        raise FileNotFoundError(f"system_prompt.md not found at: {SYSTEM_PROMPT_PATH}")

    with open(SYSTEM_PROMPT_PATH, "r", encoding="utf-8") as f:
        system_prompt = f.read()

    # å…ˆè™•ç† frames_dictsï¼Œç¢ºä¿æ¯å€‹å¹€éƒ½æœ‰ caption
    del_list = ["frame", "is_not_blurry", "is_significant", "variance", "ssim_value", "mse_value"]
    # è·³éis_not_blurryèˆ‡is_significant
    new_frames_dicts = []
    for item in frames_dicts:
        if item["is_not_blurry"] and item["is_significant"]:
            new_frames_dicts.append(item)

    cleaned_frames = [
        {k: v for k, v in fr.items() if k not in del_list}
        for fr in new_frames_dicts
    ]
   
    
    frames_summary = [
        {
            "index": i,
            "stamp": fr.get("stamp"),
            "caption": fr.get("caption", "")
        }
        for i, fr in enumerate(cleaned_frames)
        ]
    # æº–å‚™ prompt
    prompt = {
        "system_prompt": str(system_prompt),
        "describe": str(frames_summary)
    }

    # å¦‚æœè¼¸å‡ºå¤±æ•—ï¼Œæˆ–è€…ç„¡æ³•è½‰æ›æˆ JSONï¼Œå‰‡é‡è©¦ number_of_trys æ¬¡
    output_num = number_of_trys
    while number_of_trys != 0:
        result = LLM_CORE.invoke(text = str(prompt))
        # æ¸…ç†æ¨¡å‹è¼¸å‡º
        result = clean_model_output(result)
        if result:
            break
        number_of_trys -= 1
    else:
        raise RuntimeError(f"æ¨¡å‹å˜—è©¦è¶…éè¦å®š{output_num}æ¬¡éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥æ¨¡å‹è¼¸å‡ºæˆ–é‡è©¦ã€‚")

    # æ‹¼è£å›åŸå§‹æ ¼å¼
    return result, frames_summary
# ---- å·¥å…·ï¼šè§£æ ISO æ™‚é–“ï¼ˆå…è¨± Noneï¼‰ ----
def _parse_iso_dt(s: Optional[str]) -> Optional[datetime]:
    if not s:
        return None
    try:
        # æ”¯æ´ "Z"
        return datetime.fromisoformat(s.replace("Z", "+00:00"))
    except Exception:
        return None

# ---- å¾ LLM å›å‚³æŠ“å‡º eventsï¼ˆå„ªå…ˆ final_answer.eventsï¼Œå…¶æ¬¡ rounds[*].events èšåˆï¼‰----
def _pick_llm_events(payload: dict) -> List[dict]:
    final = payload.get("final_answer", {})
    evs = final.get("events", [])
    if isinstance(evs, list) and evs:
        return evs
    rounds = payload.get("rounds", []) or []
    agg = []
    for r in rounds:
        revents = r.get("events", [])
        if isinstance(revents, list):
            agg.extend(revents)
    return agg

# ---- index â†’ ç§’æ•¸ï¼šä½¿ç”¨ frames_summary çš„ stampï¼›å¤¾é™ä¸¦å›å‚³æ˜¯å¦æœ‰å¤¾é™ ----
def _idx_to_time_from_summary(frames_summary: List[Dict[str, Any]], idx: int) -> Tuple[float, bool]:
    """
    å›å‚³ (stamp_in_seconds, clamped_flag)
    """
    if not frames_summary:
        return 0.0, False
    n = len(frames_summary)
    orig = idx
    i = max(0, min(int(idx), n - 1))
    clamped = (i != orig)
    ts = frames_summary[i].get("stamp", 0.0)
    try:
        return float(ts), clamped
    except Exception:
        return 0.0, clamped

# ---- ç”¨ index å¼·åˆ¶æ˜ å°„æˆ EventItem çµæ§‹éœ€è¦çš„æ¬„ä½ ----
def _build_events_from_llm_by_index(llm_result: dict,
                                    frames_summary: List[Dict[str, Any]],
                                    *,
                                    epsilon: float = 1e-3) -> Tuple[List[Dict[str, Any]], int]:
    """
    åƒ…ä½¿ç”¨ start_index/end_indexï¼›å®Œå…¨å¿½ç•¥ä»»ä½• start_time/end_timeã€‚
    ç”¢å‡ºç¬¦åˆ EventItem çš„ dict æ¸…å–®ï¼ˆä¸ä¾è³´ Pydantic é¡åˆ¥ï¼Œæ–¹ä¾¿åºåˆ—åŒ–ï¼‰ã€‚
    å›å‚³ (events, clamp_count)ï¼Œclamp_count è¡¨ç¤ºç´¢å¼•è¢«å¤¾é™çš„æ¬¡æ•¸ï¼ˆé™¤éŒ¯ç”¨ï¼‰ã€‚
    """
    raw_events = _pick_llm_events(llm_result)
    events: List[Dict[str, Any]] = []
    clamp_count = 0

    for ev in raw_events:
        if "start_index" not in ev or "end_index" not in ev:
            # æ²’æœ‰ç´¢å¼•å°±è·³é
            continue
        try:
            s_idx = int(ev["start_index"])
            e_idx = int(ev["end_index"])
        except Exception:
            continue

        # é †åºä¿®æ­£
        if s_idx > e_idx:
            s_idx, e_idx = e_idx, s_idx

        # ç´¢å¼•è½‰æ™‚é–“ï¼ˆå¤¾é™ï¼‰
        st, c1 = _idx_to_time_from_summary(frames_summary, s_idx)
        et, c2 = _idx_to_time_from_summary(frames_summary, e_idx)
        clamp_count += int(c1) + int(c2)

        # å®‰å…¨è™•ç†ï¼šé¿å… 0 é•·åº¦äº‹ä»¶
        if et < st:
            st, et = et, st
        if abs(et - st) < epsilon:
            et = st + epsilon

        # çµ„è£æˆ EventItem å°æ‡‰çš„ dict
        try:
            events.append({
                "start_time": float(st),
                "end_time": float(et),
                "summary": str(ev.get("summary", "")).strip() or "(no summary)",
                "objects": list(ev.get("objects", []) or []),
                "scene": ev.get("scene"),
                "action": ev.get("action"),
            })
        except Exception:
            # å–®ç­†å£æ‰å°±ç•¥é
            continue

    return events, clamp_count

# ---- æ”¶é›† metricsï¼ˆå«å¹€è™•ç†èˆ‡ LLM äº‹ä»¶çµ±è¨ˆï¼‰----
def _collect_metrics(video_info: dict,
                     frames_with_flags: List[Dict[str, Any]],
                     frames_summary: List[Dict[str, Any]],
                     *,
                     llm_events_count: int = 0,
                     index_clamp_count: int = 0) -> Dict[str, Any]:
    total = len(frames_with_flags)
    not_blurry = sum(1 for x in frames_with_flags if x.get("is_not_blurry"))
    significant = sum(1 for x in frames_with_flags if x.get("is_significant"))
    captioned = sum(
        1 for x in frames_with_flags
        if isinstance(x.get("caption"), str)
        and x["caption"]
        and x["caption"] != "<skipped due to blur or insignificance>"
    )
    kept_for_llm = len(frames_summary)

    return {
        # å½±ç‰‡å±¤ç´š
        "video_fps": video_info.get("fps"),
        "video_total_frames": video_info.get("total_frames"),
        "video_duration_sec": video_info.get("duration"),
        "target_fps": video_info.get("target_frame"),
        "effective_fps": video_info.get("effective_fps"),
        "extracted_frames": video_info.get("extracted_frames"),
        "possible_extracts": video_info.get("possible_extracts"),

        # å¹€è™•ç†çµ±è¨ˆ
        "frames_total": total,
        "frames_not_blurry": not_blurry,
        "frames_significant": significant,
        "frames_captioned": captioned,
        "frames_kept_for_llm": kept_for_llm,
        "not_blurry_rate": (not_blurry / total) if total else 0.0,
        "significant_rate": (significant / total) if total else 0.0,
        "captioned_rate": (captioned / total) if total else 0.0,

        # LLM çµæœçµ±è¨ˆ
        "llm_events_count": llm_events_count,
        "index_clamp_count": index_clamp_count,
    }

# ---- å»ºç«‹æˆåŠŸï¼å¤±æ•—çš„ JobResult dictï¼ˆä¸ç¶å®š Pydanticï¼Œæ–¹ä¾¿ Celery å›å‚³ï¼‰----
def _make_success_jobresult(job: dict,
                            video_info: dict,
                            events: List[Dict[str, Any]],
                            frames_with_flags: List[Dict[str, Any]],
                            frames_summary: List[Dict[str, Any]]) -> Dict[str, Any]:
    # video_start_time / video_end_time æ¨ç®—
    video_start_dt = _parse_iso_dt(job.get("params", {}).get("video_start_time"))
    video_end_dt = None
    try:
        dur = float(video_info.get("duration")) if video_info.get("duration") is not None else None
    except Exception:
        dur = None
    if video_start_dt and isinstance(dur, float):
        video_end_dt = video_start_dt + timedelta(seconds=dur)

    metrics = _collect_metrics(
        video_info, frames_with_flags, frames_summary,
        llm_events_count=len(events), index_clamp_count=0  # é€™è£¡çš„ clamp è¨ˆæ•¸å¾å¤–å±¤ä¸Ÿ
    )

    return {
        "job_id": job.get("job_id", "?"),
        "trace_id": job.get("trace_id"),
        "status": "success",  # JobStatus.SUCCESS
        "video_start_time": video_start_dt.isoformat() if video_start_dt else None,
        "video_end_time": video_end_dt.isoformat() if video_end_dt else None,
        "error_code": None,
        "error_message": None,
        "duration": None, # ä»»å‹™é‹è¡Œæ™‚é–“
        "metrics": metrics,
        "events": events,
    }

def _make_failed_jobresult(job: dict,
                           video_info: Optional[dict],
                           *,
                           code: str,
                           message: str,
                           frames_with_flags: Optional[List[Dict[str, Any]]] = None,
                           frames_summary: Optional[List[Dict[str, Any]]] = None,
                           raw_llm: Optional[dict] = None,
                           index_clamp_count: int = 0) -> Dict[str, Any]:
    video_start_dt = _parse_iso_dt(job.get("params", {}).get("video_start_time"))
    metrics = None
    if video_info is not None:
        metrics = _collect_metrics(
            video_info,
            frames_with_flags or [],
            frames_summary or [],
            llm_events_count=0,
            index_clamp_count=index_clamp_count,
        )
        if raw_llm is not None:
            metrics["raw_llm_result"] = raw_llm

    return {
        "job_id": job.get("job_id", "?"),
        "trace_id": job.get("trace_id"),
        "status": "failed",  # JobStatus.FAILED
        "video_start_time": video_start_dt.isoformat() if video_start_dt else None,
        "video_end_time": None,
        "error_code": code,
        "error_message": message,
        "duration": None,
        "metrics": metrics,
        "events": [],
    }

def video_description_extraction_main(job: dict):
    """
    step 1 : å¾ job å–å¾— video_url
    step 2 : å°‡å½±ç‰‡è®€å–è‡³è¨˜æ†¶é«” (opencv)ï¼Œä¸¦å£“ç¸®å¤§å°(å…ˆç•¥éï¼Œç­‰å¾ŒçºŒå„ªåŒ–)
    step 3 : å°‡å½±ç‰‡åˆ†å‰²æˆå¹€(æŠ½å¹€ï¼Œ3ç§’ä¸€å¹€)ï¼Œä¸¦åšæˆ {"stamp": "å¹€çš„ç›¸å°æ™‚é–“", "frame": å¹€åœ–ç‰‡} çš„dictæ ¼å¼
    step 4 : å»é™¤è³‡è¨Šé‡éä½çš„å¹€(æ¨¡ç³Šçš„ã€å–®è‰²ç„¡æ˜é¡¯é‚Šç·£çš„)
    step 5 : å°‡å¹€èˆ‡ä¸Šä¸€æ­¥çš„å¹€åšå·®ç•°æ¯”å°ï¼Œéæ¿¾æ‰èˆ‡å‰ä¸€å¹€å·®ç•°éå°çš„å¹€ (å…ˆç•¥éï¼Œç­‰å¾ŒçºŒå„ªåŒ–)
    step 6 : é€é MDL.BLIPImageCaptionerè¼‰å…¥Captioner Model
    step 7 : å°‡å‰©é¤˜çš„å¹€é€å…¥Captioner Modelï¼Œå–å¾—æ¯ä¸€å¹€çš„æè¿°
    step 8 : å°‡æ¯ä¸€å¹€çš„æè¿°èˆ‡æ™‚é–“æˆ³æ”¾å…¥promptä¸­ï¼Œçµ„æˆå®Œæ•´çš„prompt
    step 9 : å°‡prompté€å…¥MDL.llm_coreï¼Œå–å¾—æœ€çµ‚çš„æè¿°
    step 10: å°‡çµæœæ•´ç†æˆå°æ‡‰æ ¼å¼ï¼Œå‘¼å«API Serverï¼Œè®“çµæœå­˜å…¥è³‡æ–™åº«
    (æ„Ÿè¦ºç¼ºäº†ä¸€å€‹æ¨¡å‹å­˜æ´»æ™‚é–“æ§ç®¡çš„æ¨¡å¡Šï¼Œéœ€è¦ç ”ç©¶å¦‚ä½•è®“æœ‰ä»»å‹™çš„ç‹€æ…‹ä¸‹ä¸é‡è¤‡è¼‰å…¥æ¨¡å‹ï¼Œç›´åˆ°æ‰€æœ‰ä»»å‹™åšå®Œå¾Œ1åˆ†é˜å†é‡‹æ”¾æ¨¡å‹è¨˜æ†¶é«”)

    """
    try:
        # === Step 1~3: å–å¹€ ===
        reply = get_video_frames_fast(
            video_url=job.get("input_url", ""),
            target_fps=job.get("params", {}).get("target_fps", 3)
        )
        video_info = reply["video_info"]
        frames = reply["frames"]

        # === Step 4: æ¨¡ç³Šåº¦éæ¿¾ ===
        reply = analyze_blur(
            frames_dicts=frames,
            threshold=job.get("params", {}).get("blur_threshold", 20.0)
        )

        # === Step 5: å¹€å·®éæ¿¾ ===
        reply = filter_by_frame_difference(
            frames_dicts=reply,
            threshold=job.get("params", {}).get("difference_threshold", 0.8),
            compression_proportion=job.get("params", {}).get("compression_proportion", 0.5),
            module=job.get("params", {}).get("difference_module", "SSIM")
        )

        # === Step 6~7: Caption ===
        reply = img_captioning(reply)

        # === Step 8~9: LLM ===
        llm_result, frames_summary = llm_processing(reply)

        # å®‰å…¨æª¢æŸ¥ï¼šframes_summary å¿…é ˆå­˜åœ¨ä¸”éç©ºï¼Œå¦å‰‡ç„¡æ³•åš indexâ†’ç§’
        if not isinstance(frames_summary, list) or len(frames_summary) == 0:
            return _make_failed_jobresult(
                job, video_info,
                code="INDEX_MAPPING_EMPTY",
                message="frames_summary ç‚ºç©ºï¼Œç„¡æ³•å¾ index æ˜ å°„æ™‚é–“æˆ³ã€‚",
                frames_with_flags=reply,
                frames_summary=frames_summary,
                raw_llm=llm_result
            )

        # === å¼·åˆ¶ç”¨ indexâ†’ç§’æ•¸æ˜ å°„ï¼Œå®Œå…¨å¿½ç•¥ä»»ä½• start_time/end_time ===
        events, clamp_count = _build_events_from_llm_by_index(llm_result, frames_summary)

        if not events:
            return _make_failed_jobresult(
                job, video_info,
                code="INVALID_LLM_EVENTS",
                message="LLM å›å‚³ç¼ºå°‘æœ‰æ•ˆçš„ start_index/end_indexã€‚",
                frames_with_flags=reply,
                frames_summary=frames_summary,
                raw_llm=llm_result
            )

        # æˆåŠŸï¼šçµ„è£ JobResult
        jr = _make_success_jobresult(job, video_info, events, reply, frames_summary)
        # è£œå›å¤¾é™çµ±è¨ˆ
        if isinstance(jr.get("metrics"), dict):
            jr["metrics"]["index_clamp_count"] = clamp_count

        return jr

    except Exception as e:
        # å ±éŒ¯çš„ç¨‹å¼è¿”å›æ ¼å¼
        return _make_failed_jobresult(
            job, video_info if "video_info" in locals() else None,
            code=getattr(e, "__class__", type(e)).__name__,
            message=str(e)
        )
    
import requests
API_SERVER_URL = "http://localhost:8000"
headers = {
    "X-API-Key": "aQV0OW43EmgRbQkOeDEJCT4QX8ZaZShQdHCQKYTyJsy8Z0n_9HIeiARXTAUkjw7Q",
    "Content-Type": "application/json"
}

@app.task(name="tasks.video_description_extraction", bind=True, acks_late=True)
def video_description_extraction(self, job: dict):
    start_time = time.time()
    reply = video_description_extraction_main(job)
    end_time = time.time()
    duration = end_time - start_time
    reply["duration"] = duration
    # å‘¼å« API Serverï¼Œå°‡çµæœå­˜å…¥è³‡æ–™åº«
    # print("URLï¼š"+job.get('input_url', ''))
    # print(reply)
    try:
        response = requests.post(
            f"{API_SERVER_URL}/jobs/{job.get('job_id', '?')}/complete",
            headers=headers,
            json=reply,
            timeout=30
        )
        response.raise_for_status()
        return response.json()
    
    except requests.RequestException as e:
        # API å‘¼å«å¤±æ•—ï¼Œå›å‚³éŒ¯èª¤è¨Šæ¯
        return {
            "job_id": job.get("job_id", "?"),
            "trace_id": job.get("trace_id"),
            "error_code": "API_CALL_FAILED",
            "error_message": str(e)
        }

