import threading
import numpy as np
import jieba
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
import torch
import os

class RAGModel:
    """
    RAG Embedding æ¨¡å‹å–®ä¾‹ç®¡ç†å™¨
    é¡ä¼¼ BLIP çš„å•Ÿå‹•æ§ç®¡,ç¢ºä¿æ¨¡å‹åªè¼‰å…¥ä¸€æ¬¡
    """
    _instance = None
    _lock = threading.Lock()
    _initialized = False
    
    def __init__(self):
        if RAGModel._initialized:
            return
            
        print("[RAG] ğŸ” æ­£åœ¨è¼‰å…¥ Embedding æ¨¡å‹: intfloat/multilingual-e5-large ...")
        
        # è¨­ç½®ç·©å­˜ç›®éŒ„
        cache_dir = os.getenv("HF_HOME", "./adapters/.cache/huggingface")
        os.makedirs(cache_dir, exist_ok=True)
        
        # è‡ªå‹•æª¢æ¸¬è¨­å‚™
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"[RAG] ä½¿ç”¨è¨­å‚™: {device}")
        
        # è¼‰å…¥æ¨¡å‹
        self.model = SentenceTransformer(
            "intfloat/multilingual-e5-large",
            cache_folder=cache_dir,
            device=device
        )
        
        RAGModel._initialized = True
        print(f"[RAG] âœ… Embedding æ¨¡å‹å·²è¼‰å…¥è‡³ {device}")

    @classmethod
    def get_instance(cls):
        """ç²å– RAG æ¨¡å‹å–®ä¾‹"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    print("[RAG] é¦–æ¬¡åˆå§‹åŒ– RAG æ¨¡å‹...")
                    cls._instance = cls()
        return cls._instance
    
    @classmethod
    def is_loaded(cls):
        """æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²è¼‰å…¥"""
        return cls._initialized
    
    def encode(self, texts: list[str], **kwargs) -> list[list[float]]:
        # E5 requires "passage: " prefix for documents and "query: " for queries.
        # We will handle prefixing outside or allow caller to specify.
        embeddings = self.model.encode(texts, normalize_embeddings=True, **kwargs)
        return embeddings

    def similarity(self, query_embeddings, chunk_embeddings):
        return self.model.similarity(query_embeddings, chunk_embeddings)

def create_bm25(chunks: list[str]) -> BM25Okapi:
    # jieba.cut returns a generator, we need list of tokens
    tokenized_chunks = [list(jieba.cut(chunk)) for chunk in chunks]
    return BM25Okapi(tokenized_chunks)

def bm25_retrieve(query: str, chunks: list[str], bm25_obj: BM25Okapi) -> list[float]:
    tokenized_query = list(jieba.cut(query))
    scores = bm25_obj.get_scores(tokenized_query)
    return scores.tolist()

def reciprocal_rank_fusion(ranked_lists: list[list[int]], k=60) -> list[int]:
    """
    ranked_lists: List of lists, where each list contains item IDs (or indices) in ranked order.
    """
    scores = {}
    for rl in ranked_lists:
        for rank, doc_id in enumerate(rl, start=1):
            scores[doc_id] = scores.get(doc_id, 0.0) + 1.0 / (k + rank)
    
    fused = sorted(scores.items(), key=lambda x: (-x[1], x[0]))
    return [d for d, _ in fused]

