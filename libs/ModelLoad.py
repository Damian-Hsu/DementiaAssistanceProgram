import os
import json

class llm_core:
    def __init__(self, supplier, model_name, api_key=""):
        """
        supplier => google, openai
        """
        self.supplier = supplier.lower()
        self.model_name = model_name
        self.api_key = api_key

        if self.supplier == "google":
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(model_name=model_name)

        elif self.supplier == "openai":
            import openai
            openai.api_key = api_key
            self.openai = openai  # ä¿ç•™ç‰©ä»¶ä¾›å¾ŒçºŒå‘¼å«

        else:
            raise ValueError(f"Unsupported supplier: {supplier}")

    def invoke(self, text):
        if self.supplier == "google":
            response = self.model.generate_content(text)
            return response.text

        elif self.supplier == "openai":
            response = self.openai.ChatCompletion.create(
                model=self.model_name,
                messages=[{"role": "user", "content": text}],
                max_tokens=1000
            )
            return response.choices[0].message.content

        else:
            raise ValueError("Unsupported supplier")

    def run_chat(self, user_input, chat_filename, max_turns=10):
        """
        ä¿ç•™å°è©±ä¸Šä¸‹æ–‡ä¸¦å¯«å…¥ chatdata è³‡æ–™å¤¾ï¼Œä»¥ JSON æ ¼å¼å„²å­˜ï¼Œæª”æ¡ˆç”±ä½¿ç”¨è€…æŒ‡å®šåç¨±ã€‚
        ä¸ä¿ç•™å°è©±æ–¼è¨˜æ†¶é«”ä¸­ï¼Œåƒ…ä¾é è®€å¯« JSONã€‚
        max_turns => æœ€å¤šä¿ç•™å¤šå°‘è¼ªå°è©±ï¼ˆ1è¼ª = user + assistantï¼‰
        """
        os.makedirs("chatdata", exist_ok=True)
        full_path = os.path.join("chatdata", f"{chat_filename}.json")
        config_path = os.path.join("chatdata", f"{chat_filename}_config.json")

        # è‹¥ä¸å­˜åœ¨ï¼Œå»ºç«‹å°æ‡‰çš„ config æª”æ¡ˆ
        if not os.path.exists(full_path):
            with open(config_path, "w", encoding="utf-8") as f:
                json.dump({"supplier": self.supplier}, f, ensure_ascii=False, indent=2)

        # è®€å–ç¾æœ‰å°è©±ç´€éŒ„
        if os.path.exists(full_path):
            with open(full_path, "r", encoding="utf-8") as f:
                chat_history = json.load(f)
        else:
            chat_history = []

        # æ–°å¢ä½¿ç”¨è€…è¼¸å…¥èˆ‡å›æ‡‰ï¼Œä¸¦é™åˆ¶å°è©±é•·åº¦ï¼ˆåªä¿ç•™æœ€è¿‘ max_turns è¼ªï¼‰
        chat_history.append({"role": "user", "content": user_input})

        # ä¿ç•™æœ€è¿‘ max_turns è¼ªï¼ˆæ¯è¼ª 2 å‰‡è¨Šæ¯ï¼‰
        if len(chat_history) > max_turns * 2:
            chat_history = chat_history[-max_turns * 2:]

        # å‘¼å« LLM ä¸¦å–å¾—å›è¦†
        if self.supplier == "google":
            gemini_chat = [{"role": m["role"], "parts": [m["content"]]} for m in chat_history]
            response = self.model.generate_content(gemini_chat)
            reply = response.text

        elif self.supplier == "openai":
            response = self.openai.ChatCompletion.create(
                model=self.model_name,
                messages=chat_history,
                max_tokens=1000
            )
            reply = response.choices[0].message.content
        else:
            raise ValueError("Unsupported supplier")

        # åŠ å…¥åŠ©ç†å›è¦†
        chat_history.append({"role": "assistant", "content": reply})

        # å„²å­˜æ–°çš„ JSON æª”æ¡ˆ
        with open(full_path, "w", encoding="utf-8") as f:
            json.dump(chat_history, f, ensure_ascii=False, indent=2)

        return reply

    def save_output(self, text, output_path="llm_output.txt", append=False):
        mode = "a" if append else "w"
        with open(output_path, mode, encoding="utf-8") as f:
            f.write(text + "\n")
        print(f"âœ… å·²å°‡è¼¸å‡ºå¯«å…¥ {output_path}")

        
from PIL import Image
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration

class BLIPImageCaptioner:
    
    def __init__(self, model_name="Salesforce/blip-image-captioning-base", device=None):
        print(f"ğŸ” æ­£åœ¨è¼‰å…¥ BLIP æ¨¡å‹ï¼š{model_name}")
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")

        self.processor = BlipProcessor.from_pretrained(model_name)
        self.model = BlipForConditionalGeneration.from_pretrained(model_name)
        self.model.to(self.device)

        print(f"âœ… BLIP æ¨¡å‹å·²è¼‰å…¥è‡³ {self.device}ã€‚")

    def describe(self, image_input, prompt=None):
        """
        å°‡åœ–åƒè½‰ç‚ºè‡ªç„¶èªè¨€æ•˜è¿°ã€‚

        Args:
            image_input: åœ–ç‰‡è·¯å¾‘ï¼ˆstrï¼‰æˆ– PIL.Image å°è±¡
            prompt: çµ¦æ¨¡å‹çš„æŒ‡ä»¤æç¤ºè©ï¼ˆBLIP-base ä¸éœ€è¦ï¼Œå¯ç‚º Noneï¼‰

        Returns:
            caption: åœ–åƒæè¿°æ–‡å­—ï¼ˆstrï¼‰
        """
        if isinstance(image_input, str):
            image = Image.open(image_input).convert("RGB")
        elif isinstance(image_input, Image.Image):
            image = image_input
        else:
            raise TypeError("è«‹æä¾›åœ–ç‰‡è·¯å¾‘æˆ– PIL Image ç‰©ä»¶")

        inputs = self.processor(image, return_tensors="pt").to(self.device)
        generated_ids = self.model.generate(**inputs, max_new_tokens=50)
        caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)

        return caption
    
if __name__ == '__main__':
    google_api_key = "AIzaSyBvBotMRaGYMi4YYehNTT80d5-oknnp-68"
    google_model_dict={
            1:"gemini-2.0-flash",
            2:"gemini-2.0-flash-lite",
            3:"gemma-3-27b-it"
        }
    gemini2f = llm_core("google",google_model_dict[1],google_api_key)
    print("ğŸŸ¢ å•Ÿå‹•èŠå¤©æ¨¡å¼ï¼ˆè¼¸å…¥ /exit çµæŸå°è©±ï¼‰")

    while True:
        try:
            user_input = input("ğŸ‘¤ You: ").strip()
            
            if user_input == "":
                continue  # å¿½ç•¥ç©ºç™½è¼¸å…¥

            if user_input.lower() == "/exit":
                print("ğŸ‘‹ çµæŸå°è©±ï¼Œå†è¦‹ï¼")
                break

            reply = gemini2f.run_chat(user_input=user_input, chat_filename="first")
            print("ğŸ¤– AI:", reply)

        except KeyboardInterrupt:
            print("\nâ›” ä¸­æ–·è¼¸å…¥ï¼ŒçµæŸå°è©±")
            break

        except Exception as e:
            print("âš ï¸ ç™¼ç”ŸéŒ¯èª¤ï¼š", str(e))
def add(a,b):
    return a+b